{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10756509,"sourceType":"datasetVersion","datasetId":6671722},{"sourceId":264653,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":226398,"modelId":248173}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/roman-urdu-dataset/Roman-Urdu-Poetry.csv\")\ndf = df[[\"Poetry\"]]\nprint(df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:04:07.888625Z","iopub.execute_input":"2025-02-21T12:04:07.888989Z","iopub.status.idle":"2025-02-21T12:04:08.265819Z","shell.execute_reply.started":"2025-02-21T12:04:07.888959Z","shell.execute_reply":"2025-02-21T12:04:08.264985Z"}},"outputs":[{"name":"stdout","text":"                                              Poetry\n0  aañkh se duur na ho dil se utar jā.egā \\nvaqt ...\n1  āshiqī meñ 'mīr' jaise ḳhvāb mat dekhā karo \\n...\n2  ab aur kyā kisī se marāsim baḌhā.eñ ham \\nye b...\n3  ab ke ham bichhḌe to shāyad kabhī ḳhvāboñ meñ ...\n4  ab ke tajdīd-e-vafā kā nahīñ imkāñ jānāñ \\nyaa...\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df.to_csv(\"shayari_dataset.csv\", index=False)\nprint(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:04:08.267108Z","iopub.execute_input":"2025-02-21T12:04:08.267427Z","iopub.status.idle":"2025-02-21T12:04:08.302587Z","shell.execute_reply.started":"2025-02-21T12:04:08.267396Z","shell.execute_reply":"2025-02-21T12:04:08.301758Z"}},"outputs":[{"name":"stdout","text":"                                                 Poetry\n0     aañkh se duur na ho dil se utar jā.egā \\nvaqt ...\n1     āshiqī meñ 'mīr' jaise ḳhvāb mat dekhā karo \\n...\n2     ab aur kyā kisī se marāsim baḌhā.eñ ham \\nye b...\n3     ab ke ham bichhḌe to shāyad kabhī ḳhvāboñ meñ ...\n4     ab ke tajdīd-e-vafā kā nahīñ imkāñ jānāñ \\nyaa...\n...                                                 ...\n1309  vo mere ghar nahīñ aatā maiñ us ke ghar nahīñ ...\n1310  vo mujh ko kyā batānā chāhtā hai \\njo duniyā s...\n1311  ye hai to sab ke liye ho ye zid hamārī hai \\ni...\n1312  zarā sā qatra kahīñ aaj agar ubhartā hai \\nsam...\n1313  zindagī tujh pe ab ilzām koī kyā rakkhe \\napnā...\n\n[1314 rows x 1 columns]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfile_path = \"shayari_dataset.csv\"\ndf = pd.read_csv(file_path)\nprocessed_data = []\nfor poetry in df[\"Poetry\"].dropna():  \n    lines = poetry.split(\"\\n\")\n    for i in range(0, len(lines) - 1, 2):  # Step by 2\n        processed_data.append({\"Input\": lines[i], \"Output\": lines[i + 1]})\nprocessed_df = pd.DataFrame(processed_data)\nprocessed_df.to_csv(\"shayari_dataset.csv\", index=False)\nprint(processed_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:04:09.928583Z","iopub.execute_input":"2025-02-21T12:04:09.928861Z","iopub.status.idle":"2025-02-21T12:04:10.001014Z","shell.execute_reply.started":"2025-02-21T12:04:09.928839Z","shell.execute_reply":"2025-02-21T12:04:10.000170Z"}},"outputs":[{"name":"stdout","text":"                                               Input  \\\n0            aañkh se duur na ho dil se utar jā.egā    \n1           itnā mānūs na ho ḳhalvat-e-ġham se apnī    \n2             Dūbte Dūbte kashtī ko uchhālā de duuñ    \n3            zindagī terī atā hai to ye jaane vaalā    \n4  zabt lāzim hai magar dukh hai qayāmat kā 'farāz'    \n\n                                        Output  \n0    vaqt kā kyā hai guzartā hai guzar jā.egā   \n1  tū kabhī ḳhud ko bhī dekhegā to Dar jā.egā   \n2      maiñ nahīñ koī to sāhil pe utar jā.egā   \n3   terī baḳhshish tirī dahlīz pe dhar jā.egā   \n4      zālim ab ke bhī na ro.egā to mar jā.egā  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport re\ndf = pd.read_csv('/kaggle/working/shayari_dataset.csv')\ndef clean_text(text):\n    if isinstance(text, str):\n        # Remove punctuation, commas, and special characters\n        return re.sub(r'[^\\w\\s]', '', text)\n    return text\ndf = df.applymap(clean_text)\ndf.to_csv('shayari_dataset.csv', index=False)\nprint(\"Data cleaned and saved to 'shayari_dataset.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:04:11.370524Z","iopub.execute_input":"2025-02-21T12:04:11.370815Z","iopub.status.idle":"2025-02-21T12:04:11.482028Z","shell.execute_reply.started":"2025-02-21T12:04:11.370784Z","shell.execute_reply":"2025-02-21T12:04:11.481209Z"}},"outputs":[{"name":"stdout","text":"Data cleaned and saved to 'shayari_dataset.csv'\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-4-84a17bd49328>:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df = df.applymap(clean_text)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nshayari_df = pd.read_csv('/kaggle/working/shayari_dataset.csv')\nshayari_df['Output'] = '<start> ' + shayari_df['Output'].str.replace('start ', '', regex=False).str.replace(' end', '', regex=False) + ' <end>'\nshayari_df.to_csv('shayari_dataset.csv', index=False)\nprint(\"Updated dataset saved to 'shayari_dataset.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:04:13.050510Z","iopub.execute_input":"2025-02-21T12:04:13.050827Z","iopub.status.idle":"2025-02-21T12:04:13.129237Z","shell.execute_reply.started":"2025-02-21T12:04:13.050801Z","shell.execute_reply":"2025-02-21T12:04:13.128537Z"}},"outputs":[{"name":"stdout","text":"Updated dataset saved to 'shayari_dataset.csv'\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\n# Load your dataset\nshayari_df = pd.read_csv('/kaggle/working/shayari_dataset.csv')\ndef pad_column(input_text, output_text):\n    input_tokens = input_text.split()\n    output_tokens = output_text.split()\n    max_length = max(len(input_tokens), len(output_tokens))\n    input_padded = input_tokens + ['<pad>'] * (max_length - len(input_tokens))\n    # Pad Output\n    output_padded = output_tokens + ['<pad>'] * (max_length - len(output_tokens))\n    return ' '.join(input_padded), ' '.join(output_padded)\nshayari_df[['Input', 'Output']] = shayari_df.apply(\n    lambda row: pad_column(row['Input'], row['Output']), axis=1, result_type='expand'\n)\nshayari_df.to_csv('shayari_dataset.csv', index=False)\nprint(\"Updated dataset saved to 'shayari_dataset.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:04:13.187328Z","iopub.execute_input":"2025-02-21T12:04:13.187574Z","iopub.status.idle":"2025-02-21T12:04:13.609424Z","shell.execute_reply.started":"2025-02-21T12:04:13.187554Z","shell.execute_reply":"2025-02-21T12:04:13.608666Z"}},"outputs":[{"name":"stdout","text":"Updated dataset saved to 'shayari_dataset.csv'\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import csv\nimport json\ndef tokenize_text(text):\n    \"\"\"Split text into tokens using whitespace and return as a list\"\"\"\n    return text.split()\ntokenized_data = []\nwith open('/kaggle/working/shayari_dataset.csv', 'r', encoding='utf-8') as csvfile:\n    csv_reader = csv.DictReader(csvfile)\n    for row in csv_reader:\n        try:\n            tokenized_row = {\n                'Input': tokenize_text(row['Input']),\n                'Output': tokenize_text(row['Output'])\n            }\n            tokenized_data.append(tokenized_row)\n        except KeyError as e:\n            print(f\"Missing key error: {e}. Skipping row: {row}\")\n        except Exception as e:\n            print(f\"Error processing row {row}: {e}\")\nwith open('list_tokenized_shayari.csv', 'w', encoding='utf-8', newline='') as csvfile:\n    fieldnames = ['Input', 'Output']\n    csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n    csv_writer.writeheader()\n    for row in tokenized_data:\n        csv_writer.writerow({\n            'Input': json.dumps(row['Input'], ensure_ascii=False),\n            'Output': json.dumps(row['Output'], ensure_ascii=False)\n        })\n\nprint(\"Tokenization complete. Check 'list_tokenized_shayari.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:04:13.610529Z","iopub.execute_input":"2025-02-21T12:04:13.610855Z","iopub.status.idle":"2025-02-21T12:04:13.846260Z","shell.execute_reply.started":"2025-02-21T12:04:13.610820Z","shell.execute_reply":"2025-02-21T12:04:13.845481Z"}},"outputs":[{"name":"stdout","text":"Tokenization complete. Check 'list_tokenized_shayari.csv'\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport ast\ndata_path = \"list_tokenized_shayari.csv\"\ndf = pd.read_csv(data_path, header=None, names=[\"Input\", \"Output\"])\nvocab = set()\nfor col in [\"Input\", \"Output\"]:\n    for text in df[col]:\n        try:\n            words = ast.literal_eval(text)  # Safely convert string to list\n            vocab.update(words)\n        except (SyntaxError, ValueError):\n            print(f\"Skipping malformed entry: {text}\")\nspecial_tokens = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<UNK>\": 3}\nvocab.discard(\"<pad>\")\nvocab.discard(\"<start>\")\nvocab.discard(\"<end>\")\nvocab.discard(\"<UNK>\")\nword_to_index = {word: idx for idx, word in enumerate(sorted(vocab), start=4)}\nword_to_index = {**special_tokens, **word_to_index}\njson_path = \"vocabulary.json\"\nwith open(json_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(word_to_index, f, ensure_ascii=False, indent=4)\nprint(f\"Vocabulary saved to {json_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:04:16.220141Z","iopub.execute_input":"2025-02-21T12:04:16.220452Z","iopub.status.idle":"2025-02-21T12:04:16.711350Z","shell.execute_reply.started":"2025-02-21T12:04:16.220429Z","shell.execute_reply":"2025-02-21T12:04:16.710671Z"}},"outputs":[{"name":"stdout","text":"Skipping malformed entry: Input\nSkipping malformed entry: Output\nVocabulary saved to vocabulary.json\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import json\n\n# Load vocabulary from the JSON file\nwith open('vocabulary.json', 'r', encoding='utf-8') as f:\n    vocab_dict = json.load(f)\nword = \"aarzoo\"\nword_index = vocab_dict.get(word, None)  \nprint(f\"Index of '{word}': {word_index}\")\nreverse_vocab_dict = {v: k for k, v in vocab_dict.items()}\nindices = [3, 4, 5, 6]  # Example word indices\ndecoded_words = [reverse_vocab_dict.get(idx, \"<UNK>\") for idx in indices]\nprint(f\"Decoded words: {' '.join(decoded_words)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:04:17.774831Z","iopub.execute_input":"2025-02-21T12:04:17.775137Z","iopub.status.idle":"2025-02-21T12:04:17.792160Z","shell.execute_reply.started":"2025-02-21T12:04:17.775115Z","shell.execute_reply":"2025-02-21T12:04:17.791355Z"}},"outputs":[{"name":"stdout","text":"Index of 'aarzoo': None\nDecoded words: <UNK> DaTā Daakā Daal\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import csv\nimport json\nvocab_path = 'vocabulary.json'\ntokenized_data_path = 'list_tokenized_shayari.csv'\noutput_csv_path = 'shayari_sequences.csv'\nwith open(vocab_path, 'r', encoding='utf-8') as f:\n    vocabulary = json.load(f)\ndef text_to_sequence(text, vocab):\n    sequence = []\n    for word in text:\n        word_cleaned = word.strip()  \n        if word_cleaned in vocab:\n            sequence.append(vocab[word_cleaned])\n        else:\n            print(f\"Unmatched token: '{word_cleaned}'\") \n            sequence.append(-1)\n    return sequence\nwith open(tokenized_data_path, 'r', encoding='utf-8') as csvfile:\n    reader = csv.DictReader(csvfile)\n    output_data = []\n    for row in reader:\n        input_text = eval(row['Input'])  \n        output_text = eval(row['Output'])\n        input_sequence = text_to_sequence(input_text, vocabulary)\n        output_sequence = text_to_sequence(output_text, vocabulary)\n        output_data.append({\n            'Input': input_sequence,\n            'Output': output_sequence\n        })\nwith open(output_csv_path, 'w', encoding='utf-8', newline='') as csvfile:\n    fieldnames = ['Input', 'Output']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in output_data:\n        writer.writerow({\n            'Input': row['Input'],\n            'Output': row['Output']\n        })\nprint(f\"Converted sequences have been saved to {output_csv_path}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:04:19.303476Z","iopub.execute_input":"2025-02-21T12:04:19.303780Z","iopub.status.idle":"2025-02-21T12:04:19.966021Z","shell.execute_reply.started":"2025-02-21T12:04:19.303754Z","shell.execute_reply":"2025-02-21T12:04:19.965264Z"}},"outputs":[{"name":"stdout","text":"Converted sequences have been saved to shayari_sequences.csv.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset,Dataset\nimport pandas as pd\nimport torch\nimport ast\nfrom torch.nn.utils.rnn import pad_sequence\nclass DataLoad(Dataset):\n  def __init__(self, file_path):\n    df = pd.read_csv(file_path)\n    self.inputs = [ast.literal_eval(x) for x in df['Input']]\n    self.outputs = [ast.literal_eval(x) for x in df['Output']]\n  def __len__(self):\n    return len(self.inputs)\n  def __getitem__(self,idx):\n    input_tensor = torch.tensor(self.inputs[idx], dtype=torch.int64)\n    output_tensor = torch.tensor(self.outputs[idx],dtype=torch.int64)\n    return input_tensor, output_tensor\ndef Add_Pad(batch):\n  inputs,outputs = zip(*batch)\n  inputs = pad_sequence(inputs,batch_first=True,padding_value=0)\n  outputs = pad_sequence(outputs,batch_first=True,padding_value=0)\n  return inputs,outputs\ndataset = DataLoad('shayari_sequences.csv')\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True,collate_fn=Add_Pad)\ndata_iter = iter(dataloader)\nfeatures, labels = next(data_iter)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:04:21.346765Z","iopub.execute_input":"2025-02-21T12:04:21.347066Z","iopub.status.idle":"2025-02-21T12:04:25.017818Z","shell.execute_reply.started":"2025-02-21T12:04:21.347043Z","shell.execute_reply":"2025-02-21T12:04:25.017157Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(\"Features:\\n\", features)\nprint(\"Labels:\\n\", labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:04:25.018922Z","iopub.execute_input":"2025-02-21T12:04:25.019202Z","iopub.status.idle":"2025-02-21T12:04:25.034029Z","shell.execute_reply.started":"2025-02-21T12:04:25.019174Z","shell.execute_reply":"2025-02-21T12:04:25.033083Z"}},"outputs":[{"name":"stdout","text":"Features:\n tensor([[ 2889,  7202,  7019,  9427,  5652,   502,  7427, 12987,  9499,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 4257, 12727,  5652,  4613,  8684,  7202,  2881, 12021,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [15857, 14494,  5474,  5241,  7398,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 9874, 10262,  8581,  8792, 15257,   846, 15257, 16388,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [11783,  5208, 16404,  7488, 11782, 10350,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 6111,  7039, 12727,  7427,  5652,  7202, 15353,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 9600,  8792,  3081,  5795,  8258,  2143, 14650,  2143,  5662,  9499,\n             0,     0,     0,     0,     0,     0],\n        [10763,  7398, 10763,  9427,  6959, 13488, 15089,  7483,  6959,     0,\n             0,     0,     0,     0,     0,     0],\n        [15294, 13425, 10464,  1215,  5178,   678, 11304, 13680,  5178,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 8686,  7488,  4257,  3866,  9002,  5178,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 8621, 12727,  2748,  5178,  5085,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 6898,  6031, 17025,  7587, 13978,  7587,  8961,  9427,  8818,     0,\n             0,     0,     0,     0,     0,     0],\n        [12400, 10278,  7398, 10279,  5178,  8119, 11810, 11810,  7083,     0,\n             0,     0,     0,     0,     0,     0],\n        [10902, 11572,  8792,   831,  9796,  8792,  4391,  6116,  5178,     0,\n             0,     0,     0,     0,     0,     0],\n        [15055,  7587,  2005,  7488, 15526,  5178, 15987, 12083,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [  685, 16337,  3213,  7108, 14548, 16571,  8792,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [14640, 12727,  5609, 14617, 10667,  6034,  7398,  7430,  6954,  7587,\n             0,     0,     0,     0,     0,     0],\n        [ 5888, 12727,  2143,  5178, 13470,  7430, 14938,  7587,  8277,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 3528,  5178, 10735,  5333,  4257,  5015, 10464, 16822,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 3696,  9804,  6031,  1614,  8792,  2162, 14617,  3878,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [10070,  6031, 11601, 12727, 17205,   134,  7718,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 7083,  7202,  2607,  4138,  3532, 14847,  9804,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [12447, 14608,  7587, 15363, 11164,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [10685,  8792,   483,  9499, 11458,  9427,  4353,  7398,  9437,     0,\n             0,     0,     0,     0,     0,     0],\n        [15017,  6970, 11445,   968,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 3091,  5178, 10735,  7380,  7398,  9100,  8792,   266,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [14651,   405,  7398,  8258,   391,  7483, 12347,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 6970,  7488, 12978,  6970,  7488, 16502, 14180,  3922, 13618,  5178,\n         15526,  8588,     0,     0,     0,     0],\n        [ 7370, 10504,  8792,  5178, 16089,   482, 16914,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [11122, 12727, 14496,  6970,  6743,  5241,  1750,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 3500, 12727, 10735,   482, 17015,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 6615, 13668, 14494,  3226, 13736, 15471,   998,     0,     0,     0,\n             0,     0,     0,     0,     0,     0]])\nLabels:\n tensor([[    1,  9880, 14597,  6423,  9499,     2,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1,  8784, 16338, 12727,  6972,  7271,  4754,  8352,  8781,     2,\n             0,     0,     0,     0,     0,     0],\n        [    1,  7430,  9427,  7430, 11305, 10670,  7873,   220,  5208,     2,\n             0,     0,     0,     0,     0,     0],\n        [    1, 15257, 11260, 15257,  4540, 15257, 15569, 15257, 13981,     2,\n             0,     0,     0,     0,     0,     0],\n        [    1, 15526,  5561,  9044, 12727,  5555,  7114,  5208,     2,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1,  6111,  7039,  8792,  2960,  4754,  5178,  6972,     2,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1,  1688,  8692,  5652,   383, 14552,  3992, 12025,  2672,     2,\n             0,     0,     0,     0,     0,     0],\n        [    1,  7483,  5804, 16404,  9804,  3175, 10670, 12240, 11475,     2,\n             0,     0,     0,     0,     0,     0],\n        [    1, 12670,  3573,  7398, 10298,  5208, 13866,  7202, 14980,  7939,\n             2,     0,     0,     0,     0,     0],\n        [    1,  9907,  7482, 11318,  2065,  9499,   276,     2,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1,  5178, 14369,  8792, 15829,  6303,  8652,     2,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1, 13148, 14959,  9720,   220,  9045,  5026,  7202,  1729,     2,\n             0,     0,     0,     0,     0,     0],\n        [    1, 14494, 15255,  7398, 16824, 14617,  5178,     2,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1, 15089,  7149,  9499,  8834,  6033, 13785,  9499,  8833,     2,\n             0,     0,     0,     0,     0,     0],\n        [    1, 15257,  5672,  6615, 11069,  8792, 14617, 10735,  6889,  7398,\n         16432,  5673,     2,     0,     0,     0],\n        [    1, 15526, 13448,  2143,  5241, 10670, 15822,  5660,  5868, 14554,\n             2,     0,     0,     0,     0,     0],\n        [    1, 16677, 13103,  9427, 12182, 13212,  7488,  6131, 14617,  5178,\n             2,     0,     0,     0,     0,     0],\n        [    1,  7348,  5241,  9804,   228, 14617,  7587, 15050, 12727,   228,\n          2143,  9427,  5805,     2,     0,     0],\n        [    1, 12076,  9862,  7488, 13813,  7394,  5695,     2,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1, 14876,  7202,  6181,  9720,  9499,   275,     2,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1,  3696,  7348, 15720, 12727,  7842, 14554,     2,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1,  6178, 12727, 10333,  3696,  7935, 14847,  9804,     2,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1, 15363, 12020, 12727,  2538,  1334,  7587, 11931,     2,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1,  5241,  5868, 16215,  8792, 15363,  6439,  2143, 14554,     2,\n             0,     0,     0,     0,     0,     0],\n        [    1, 15702,  2542,  7488,  6192, 13437, 14493,  5804,     2,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1, 13703, 12727, 14502,  3433,  7394,  5695,     2,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1,  9045, 16893,   685,  8949, 10670,  4266,  9499,     2,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1, 15467,  2143,  5909,  2503, 16968,  5178, 15256,  2143,  5909,\n         16432, 12727,  6252, 11444,  5178,     2],\n        [    1, 11407,  7348, 15144,  5178,     2,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1,  7482, 11109,  8792, 14456,  7115,  5178,  5256, 12660, 14847,\n             2,     0,     0,     0,     0,     0],\n        [    1,   242,   757,  5652,  7427,  5343, 12938,  7488,     2,     0,\n             0,     0,     0,     0,     0,     0],\n        [    1,  2092, 10302, 10035,  7488,     2,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0]])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import json\nwith open('vocabulary.json', 'r', encoding='utf-8') as f:\n    vocab_dict = json.load(f)\nVocab_size = len(vocab_dict)\nprint(f\"Vocabulary size: {Vocab_size}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:05:27.409771Z","iopub.execute_input":"2025-02-21T12:05:27.410100Z","iopub.status.idle":"2025-02-21T12:05:27.423624Z","shell.execute_reply.started":"2025-02-21T12:05:27.410075Z","shell.execute_reply":"2025-02-21T12:05:27.422969Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 17217\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nclass Seq2Seq(nn.Module):\n    def __init__(self,vocab_size,embedding_dimension,hidden_size):\n        super(Seq2Seq, self).__init__()\n        ## Encoder\n        self.Encoder_embedding = nn.Embedding(vocab_size,embedding_dimension)\n        self.batch_norm = nn.BatchNorm1d(embedding_dimension) \n        self.Encoder=nn.LSTM(embedding_dimension,hidden_size,2,batch_first=True)\n        ##Decoder\n        self.Decoder_Embedding = nn.Embedding(vocab_size,embedding_dimension)\n        self.Decoder=nn.LSTM(embedding_dimension,hidden_size,2,batch_first=True)\n        self.output_layer=nn.Linear(hidden_size,vocab_size)\n        self._initialize_weights()\n        self.norm = nn.LayerNorm(hidden_size)\n    def _initialize_weights(self):\n        for name, param in self.named_parameters():\n            if \"weight_ih\" in name:  # Input-to-hidden weights\n                init.xavier_uniform_(param, gain=nn.init.calculate_gain(\"tanh\"))\n            elif \"weight_hh\" in name:  # Hidden-to-hidden (recurrent) weights\n                init.orthogonal_(param)\n            elif \"bias\" in name:\n                nn.init.zeros_(param)\n    def forward(self,input_seq,target_seq):## the Data<---input_Data, the Target Data<---\n        #Encoder\n        Embedding_input = self.Encoder_embedding(input_seq)## -->(batch_Size,seq_len,embedding_dim)\n        Embedding_input = self.batch_norm(Embedding_input.permute(0, 2, 1)).permute(0, 2, 1)\n        encoder_output,(hidden,cell) = self.Encoder(Embedding_input)\n        #print(f\"Encoder Hidden Mean: {hidden.mean().item()}, Std Dev: {hidden.std().item()}\")\n        #print(f\"Encoder Cell Mean: {cell.mean().item()}, Std Dev: {cell.std().item()}\")\n        hidden = torch.stack([self.norm(h) for h in hidden], dim=0)\n        cell = torch.stack([self.norm(c) for c in cell], dim=0)\n        encoder_output = self.norm(encoder_output)\n        #Decoder\n        Embedding_target= self.Decoder_Embedding(target_seq)\n        Decoder_outputs,_= self.Decoder(Embedding_target,(hidden,cell))\n        # Output Layer\n        output = self.output_layer(Decoder_outputs)\n        #output = output.view(input_seq.size(0), target_seq.size(1), -1)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:04:31.748225Z","iopub.execute_input":"2025-02-21T12:04:31.748500Z","iopub.status.idle":"2025-02-21T12:04:31.755975Z","shell.execute_reply.started":"2025-02-21T12:04:31.748480Z","shell.execute_reply":"2025-02-21T12:04:31.755094Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nvocab_size = len(vocab_dict)  # Ensure vocab size is correct\nembedding_dim = 256\nhidden_size = 512\nmodel = Seq2Seq(vocab_size, embedding_dim, hidden_size).to(device)\nmodel_path = '/kaggle/input/roman-urdu-dataset/Roman-Urdu-Poetry.csv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:05:58.901942Z","iopub.execute_input":"2025-02-21T12:05:58.902230Z","iopub.status.idle":"2025-02-21T12:05:59.795405Z","shell.execute_reply.started":"2025-02-21T12:05:58.902210Z","shell.execute_reply":"2025-02-21T12:05:59.794414Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport json\nfrom tqdm import tqdm\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load vocabulary from the JSON file\nwith open('vocabulary.json', 'r', encoding='utf-8') as f:\n    vocab_dict = json.load(f)\n\n# Create reverse vocabulary for decoding\nreverse_vocab_dict = {v: k for k, v in vocab_dict.items()}\n\n# Define model parameters\nvocab_size = len(vocab_dict)  # Ensure vocab size is correct\nembedding_dim = 256\nhidden_size = 512\n\n# Initialize model\nmodel = Seq2Seq(vocab_size, embedding_dim, hidden_size).to(device)\nmodel_path = '/kaggle/input/latest-model/pytorch/default/1/seq2seq_model(8).pth'\ntry:\n    checkpoint = torch.load(model_path, map_location=device)\n    model.load_state_dict(checkpoint)\n    print(\"Loaded pre-trained model successfully!\")\nexcept FileNotFoundError:\n    print(\"No pre-trained model found. Starting training from scratch.\")\nexcept KeyError:\n    print(\"Error loading model weights. Ensure the checkpoint is correct.\")\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\nnum_epochs = 100 # Number of epochs you want to train\nfor epoch in range(num_epochs):\n    total_loss = 0\n    model.train()\n    for inputs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}\", unit=\"batch\"):\n        inputs, labels = inputs.to(device), labels.to(device)  \n        optimizer.zero_grad()\n        outputs = model(inputs, labels[:, :-1])\n        loss = criterion(outputs.view(-1, vocab_size), labels[:, 1:].contiguous().view(-1))\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}\")\n    scheduler.step()\n    model.eval()  \n    with torch.no_grad():  \n        sample_input = inputs[0].unsqueeze(0).to(device)\n        predicted_output = model(sample_input, torch.zeros_like(sample_input).to(device))\n        predicted_indices = predicted_output.argmax(dim=-1).squeeze(0).cpu().numpy()\n        predicted_words = [reverse_vocab_dict.get(idx, \"<UNK>\") for idx in predicted_indices]\n        actual_input_indices = inputs[0].cpu().numpy()\n        actual_input_words = [reverse_vocab_dict.get(idx, \"<UNK>\") for idx in actual_input_indices]\n        actual_output_indices = labels[0].cpu().numpy()\n        actual_output_words = [reverse_vocab_dict.get(idx, \"<UNK>\") for idx in actual_output_indices]\n        if epoch == 100:\n            print(\"\\n--- Sample Output at End of Epoch ---\")\n            print(f\"Actual Input: {' '.join(actual_input_words)}\")\n            print(f\"Actual Output: {' '.join(actual_output_words)}\")\n            print(f\"Predicted Output: {' '.join(predicted_words)}\")\n            print(\"------------------------------------\\n\")\n    model.train() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T17:04:48.744572Z","iopub.execute_input":"2025-02-15T17:04:48.744910Z","iopub.status.idle":"2025-02-15T17:15:07.209531Z","shell.execute_reply.started":"2025-02-15T17:04:48.744887Z","shell.execute_reply":"2025-02-15T17:15:07.208751Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-15-c30246e7f37b>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(model_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"No pre-trained model found. Starting training from scratch.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 329/329 [00:06<00:00, 47.93batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100, Loss: 6.353545965757051\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 329/329 [00:06<00:00, 53.59batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/100, Loss: 5.59351159156637\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 329/329 [00:06<00:00, 53.55batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/100, Loss: 5.091130385645255\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 329/329 [00:06<00:00, 53.52batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/100, Loss: 4.516949172440271\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 329/329 [00:06<00:00, 53.61batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/100, Loss: 3.8719079900295177\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 329/329 [00:06<00:00, 53.56batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/100, Loss: 3.1994912435943235\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 329/329 [00:06<00:00, 53.71batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/100, Loss: 2.562900607709102\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 329/329 [00:06<00:00, 53.55batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/100, Loss: 2.0330656310345265\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 329/329 [00:06<00:00, 53.50batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/100, Loss: 1.6156587694915956\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 329/329 [00:06<00:00, 53.62batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/100, Loss: 1.299303162061697\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 329/329 [00:06<00:00, 53.62batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/100, Loss: 0.984188973903656\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 329/329 [00:06<00:00, 53.58batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/100, Loss: 0.8154660097371481\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 329/329 [00:06<00:00, 53.67batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/100, Loss: 0.6910038662898866\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 329/329 [00:06<00:00, 53.77batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/100, Loss: 0.5708642933506372\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 329/329 [00:06<00:00, 53.55batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/100, Loss: 0.44841452952938604\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|██████████| 329/329 [00:06<00:00, 53.51batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/100, Loss: 0.32846938191395036\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|██████████| 329/329 [00:06<00:00, 53.56batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/100, Loss: 0.22011119050276678\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|██████████| 329/329 [00:06<00:00, 53.78batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/100, Loss: 0.13448822892364398\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|██████████| 329/329 [00:06<00:00, 53.69batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/100, Loss: 0.07622392578644956\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20: 100%|██████████| 329/329 [00:06<00:00, 53.60batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/100, Loss: 0.04275021523712797\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21: 100%|██████████| 329/329 [00:06<00:00, 53.73batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/100, Loss: 0.025239031410869493\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22: 100%|██████████| 329/329 [00:06<00:00, 53.50batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/100, Loss: 0.019304305997232717\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23: 100%|██████████| 329/329 [00:06<00:00, 53.53batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/100, Loss: 0.016169822347381558\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24: 100%|██████████| 329/329 [00:06<00:00, 53.53batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24/100, Loss: 0.01381985001311295\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25: 100%|██████████| 329/329 [00:06<00:00, 53.64batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25/100, Loss: 0.0124894973615292\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26: 100%|██████████| 329/329 [00:06<00:00, 53.72batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26/100, Loss: 0.010383192043592955\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27: 100%|██████████| 329/329 [00:06<00:00, 53.53batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27/100, Loss: 0.008730187468615709\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28: 100%|██████████| 329/329 [00:06<00:00, 53.52batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28/100, Loss: 0.007746189443456819\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29: 100%|██████████| 329/329 [00:06<00:00, 53.57batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29/100, Loss: 0.008043464005706793\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30: 100%|██████████| 329/329 [00:06<00:00, 53.54batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30/100, Loss: 0.010110207526211409\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31: 100%|██████████| 329/329 [00:06<00:00, 53.56batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31/100, Loss: 0.007651066290084442\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32: 100%|██████████| 329/329 [00:06<00:00, 53.48batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32/100, Loss: 0.005871091171034745\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33: 100%|██████████| 329/329 [00:06<00:00, 53.69batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33/100, Loss: 0.0046841226331323475\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34: 100%|██████████| 329/329 [00:06<00:00, 53.54batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34/100, Loss: 0.004181900118282319\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35: 100%|██████████| 329/329 [00:06<00:00, 53.61batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35/100, Loss: 0.0037488654506389945\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36: 100%|██████████| 329/329 [00:06<00:00, 53.73batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36/100, Loss: 0.003474273534893627\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37: 100%|██████████| 329/329 [00:06<00:00, 53.59batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37/100, Loss: 0.0036181850835828163\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38: 100%|██████████| 329/329 [00:06<00:00, 53.54batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38/100, Loss: 0.0034907840858963758\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39: 100%|██████████| 329/329 [00:06<00:00, 53.71batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39/100, Loss: 0.00289656377417293\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40: 100%|██████████| 329/329 [00:06<00:00, 53.78batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40/100, Loss: 0.0023831201322365406\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41: 100%|██████████| 329/329 [00:06<00:00, 53.63batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41/100, Loss: 0.0021679655614798734\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42: 100%|██████████| 329/329 [00:06<00:00, 53.48batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42/100, Loss: 0.002080306951819967\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43: 100%|██████████| 329/329 [00:06<00:00, 53.59batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43/100, Loss: 0.0019084294949372309\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44: 100%|██████████| 329/329 [00:06<00:00, 53.10batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44/100, Loss: 0.0017247506885118443\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45: 100%|██████████| 329/329 [00:06<00:00, 53.54batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45/100, Loss: 0.0017135786812605286\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46: 100%|██████████| 329/329 [00:06<00:00, 53.29batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46/100, Loss: 0.0016383723388383544\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47: 100%|██████████| 329/329 [00:06<00:00, 53.58batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47/100, Loss: 0.0014526803293449402\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48: 100%|██████████| 329/329 [00:06<00:00, 53.64batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48/100, Loss: 0.001414990420934481\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49: 100%|██████████| 329/329 [00:06<00:00, 53.45batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49/100, Loss: 0.0017642634881852835\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50: 100%|██████████| 329/329 [00:06<00:00, 53.78batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50/100, Loss: 0.001293445959097018\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51: 100%|██████████| 329/329 [00:06<00:00, 53.74batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51/100, Loss: 0.0011742053554169721\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52: 100%|██████████| 329/329 [00:06<00:00, 53.39batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52/100, Loss: 0.0018386279574615207\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53: 100%|██████████| 329/329 [00:06<00:00, 53.39batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53/100, Loss: 0.001054370324665222\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54: 100%|██████████| 329/329 [00:06<00:00, 53.51batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54/100, Loss: 0.001009515558134504\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55: 100%|██████████| 329/329 [00:06<00:00, 53.77batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55/100, Loss: 0.0009422521455437059\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56: 100%|██████████| 329/329 [00:06<00:00, 53.46batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56/100, Loss: 0.0009981849608379544\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57: 100%|██████████| 329/329 [00:06<00:00, 53.63batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57/100, Loss: 0.0009589807417365442\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58: 100%|██████████| 329/329 [00:06<00:00, 53.28batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58/100, Loss: 0.0008296895210311315\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59: 100%|██████████| 329/329 [00:06<00:00, 53.45batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59/100, Loss: 0.0009455519151947486\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60: 100%|██████████| 329/329 [00:06<00:00, 53.38batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 60/100, Loss: 0.0007894980985852451\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61: 100%|██████████| 329/329 [00:06<00:00, 53.42batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 61/100, Loss: 0.0007565858084595221\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62: 100%|██████████| 329/329 [00:06<00:00, 53.60batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 62/100, Loss: 0.0007893251096631618\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63: 100%|██████████| 329/329 [00:06<00:00, 53.44batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 63/100, Loss: 0.0007342350535451415\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64: 100%|██████████| 329/329 [00:06<00:00, 53.59batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 64/100, Loss: 0.0011480215745468519\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65: 100%|██████████| 329/329 [00:06<00:00, 53.59batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 65/100, Loss: 0.0007070024477637806\n","output_type":"stream"},{"name":"stderr","text":"Epoch 66: 100%|██████████| 329/329 [00:06<00:00, 53.48batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 66/100, Loss: 0.0006586149411692106\n","output_type":"stream"},{"name":"stderr","text":"Epoch 67: 100%|██████████| 329/329 [00:06<00:00, 53.72batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 67/100, Loss: 0.0007182816798072648\n","output_type":"stream"},{"name":"stderr","text":"Epoch 68: 100%|██████████| 329/329 [00:06<00:00, 53.52batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 68/100, Loss: 0.0006312058759445614\n","output_type":"stream"},{"name":"stderr","text":"Epoch 69: 100%|██████████| 329/329 [00:06<00:00, 53.75batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 69/100, Loss: 0.0006380408238350643\n","output_type":"stream"},{"name":"stderr","text":"Epoch 70: 100%|██████████| 329/329 [00:06<00:00, 53.97batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 70/100, Loss: 0.0006716798155534571\n","output_type":"stream"},{"name":"stderr","text":"Epoch 71: 100%|██████████| 329/329 [00:06<00:00, 53.77batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 71/100, Loss: 0.0005860468477971773\n","output_type":"stream"},{"name":"stderr","text":"Epoch 72: 100%|██████████| 329/329 [00:06<00:00, 53.62batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 72/100, Loss: 0.0005823680656851696\n","output_type":"stream"},{"name":"stderr","text":"Epoch 73: 100%|██████████| 329/329 [00:06<00:00, 53.63batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 73/100, Loss: 0.0005784747633965555\n","output_type":"stream"},{"name":"stderr","text":"Epoch 74: 100%|██████████| 329/329 [00:06<00:00, 53.68batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 74/100, Loss: 0.0005898208680808952\n","output_type":"stream"},{"name":"stderr","text":"Epoch 75: 100%|██████████| 329/329 [00:06<00:00, 53.70batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 75/100, Loss: 0.0005866937228905937\n","output_type":"stream"},{"name":"stderr","text":"Epoch 76: 100%|██████████| 329/329 [00:06<00:00, 53.62batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 76/100, Loss: 0.0005535322471838841\n","output_type":"stream"},{"name":"stderr","text":"Epoch 77: 100%|██████████| 329/329 [00:06<00:00, 53.78batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 77/100, Loss: 0.0006482476322723433\n","output_type":"stream"},{"name":"stderr","text":"Epoch 78: 100%|██████████| 329/329 [00:06<00:00, 53.57batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 78/100, Loss: 0.0007003095058659176\n","output_type":"stream"},{"name":"stderr","text":"Epoch 79: 100%|██████████| 329/329 [00:06<00:00, 53.63batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 79/100, Loss: 0.0005440827440343072\n","output_type":"stream"},{"name":"stderr","text":"Epoch 80: 100%|██████████| 329/329 [00:06<00:00, 53.56batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 80/100, Loss: 0.0005275566361937955\n","output_type":"stream"},{"name":"stderr","text":"Epoch 81: 100%|██████████| 329/329 [00:06<00:00, 53.72batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 81/100, Loss: 0.0005226081259666141\n","output_type":"stream"},{"name":"stderr","text":"Epoch 82: 100%|██████████| 329/329 [00:06<00:00, 53.84batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 82/100, Loss: 0.00052057438010582\n","output_type":"stream"},{"name":"stderr","text":"Epoch 83: 100%|██████████| 329/329 [00:06<00:00, 53.49batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 83/100, Loss: 0.0005130207342644842\n","output_type":"stream"},{"name":"stderr","text":"Epoch 84: 100%|██████████| 329/329 [00:06<00:00, 53.59batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 84/100, Loss: 0.000550578189904074\n","output_type":"stream"},{"name":"stderr","text":"Epoch 85: 100%|██████████| 329/329 [00:06<00:00, 53.51batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 85/100, Loss: 0.0006909538461157265\n","output_type":"stream"},{"name":"stderr","text":"Epoch 86: 100%|██████████| 329/329 [00:06<00:00, 53.69batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 86/100, Loss: 0.0005054996937363321\n","output_type":"stream"},{"name":"stderr","text":"Epoch 87: 100%|██████████| 329/329 [00:06<00:00, 53.75batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 87/100, Loss: 0.0005144728581197446\n","output_type":"stream"},{"name":"stderr","text":"Epoch 88: 100%|██████████| 329/329 [00:06<00:00, 53.56batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 88/100, Loss: 0.0005657169304428605\n","output_type":"stream"},{"name":"stderr","text":"Epoch 89: 100%|██████████| 329/329 [00:06<00:00, 53.28batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 89/100, Loss: 0.0004917907480456184\n","output_type":"stream"},{"name":"stderr","text":"Epoch 90: 100%|██████████| 329/329 [00:06<00:00, 53.58batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 90/100, Loss: 0.0004995606935549086\n","output_type":"stream"},{"name":"stderr","text":"Epoch 91: 100%|██████████| 329/329 [00:06<00:00, 53.58batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 91/100, Loss: 0.000555433518866128\n","output_type":"stream"},{"name":"stderr","text":"Epoch 92: 100%|██████████| 329/329 [00:06<00:00, 53.61batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 92/100, Loss: 0.00048651626288655886\n","output_type":"stream"},{"name":"stderr","text":"Epoch 93: 100%|██████████| 329/329 [00:06<00:00, 53.63batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 93/100, Loss: 0.0008141228958885086\n","output_type":"stream"},{"name":"stderr","text":"Epoch 94: 100%|██████████| 329/329 [00:06<00:00, 53.21batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 94/100, Loss: 0.0005174473067462489\n","output_type":"stream"},{"name":"stderr","text":"Epoch 95: 100%|██████████| 329/329 [00:06<00:00, 53.63batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 95/100, Loss: 0.0005043969371272563\n","output_type":"stream"},{"name":"stderr","text":"Epoch 96: 100%|██████████| 329/329 [00:06<00:00, 52.86batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 96/100, Loss: 0.00048118761682594167\n","output_type":"stream"},{"name":"stderr","text":"Epoch 97: 100%|██████████| 329/329 [00:06<00:00, 53.00batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 97/100, Loss: 0.0005094076716978\n","output_type":"stream"},{"name":"stderr","text":"Epoch 98: 100%|██████████| 329/329 [00:06<00:00, 53.38batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 98/100, Loss: 0.00102559532777344\n","output_type":"stream"},{"name":"stderr","text":"Epoch 99: 100%|██████████| 329/329 [00:06<00:00, 53.03batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 99/100, Loss: 0.0005093490689678689\n","output_type":"stream"},{"name":"stderr","text":"Epoch 100: 100%|██████████| 329/329 [00:06<00:00, 53.28batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 100/100, Loss: 0.0004992028267517697\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"torch.save(model.state_dict(), 'seq2seq_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:06:42.406619Z","iopub.execute_input":"2025-02-21T12:06:42.406984Z","iopub.status.idle":"2025-02-21T12:06:42.441812Z","shell.execute_reply.started":"2025-02-21T12:06:42.406955Z","shell.execute_reply":"2025-02-21T12:06:42.440674Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-4be99d1d8cee>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/kaggle/input/my_model/pytorch/default/1/seq2seq_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m             _save(\n\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: File /kaggle/input/my_model/pytorch/default/1/seq2seq_model.pth cannot be opened."],"ename":"RuntimeError","evalue":"File /kaggle/input/my_model/pytorch/default/1/seq2seq_model.pth cannot be opened.","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"model = Seq2Seq(vocab_size, embedding_dim, hidden_size)\nmodel.to(device)\nmodel.load_state_dict(torch.load('/kaggle/input/my_model/pytorch/default/1/seq2seq_model.pth'))\nmodel.eval()  # Set the model to evaluation mode for inference\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:07:22.773938Z","iopub.execute_input":"2025-02-21T12:07:22.774210Z","iopub.status.idle":"2025-02-21T12:07:24.479572Z","shell.execute_reply.started":"2025-02-21T12:07:22.774191Z","shell.execute_reply":"2025-02-21T12:07:24.478854Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-25-0ee7e11a4a2e>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/input/my_model/pytorch/default/1/seq2seq_model.pth'))\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Seq2Seq(\n  (Encoder_embedding): Embedding(17217, 256)\n  (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (Encoder): LSTM(256, 512, num_layers=2, batch_first=True)\n  (Decoder_Embedding): Embedding(17217, 256)\n  (Decoder): LSTM(256, 512, num_layers=2, batch_first=True)\n  (output_layer): Linear(in_features=512, out_features=17217, bias=True)\n  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"import torch\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ndef generate_sequence(input_seq, max_len=10):\n    # Convert input_seq (words) to indices based on vocab_dict\n    input_indices = [vocab_dict.get(word, vocab_dict.get('<UNK>')) for word in input_seq]\n    input_tensor = torch.tensor(input_indices, device=device).unsqueeze(0)  # Move to GPU\n\n    # Start the decoder with the <start> token\n    target_seq = torch.tensor([vocab_dict['<start>']], device=device).unsqueeze(0)  # Move to GPU\n    generated_sequence = []\n    hidden, cell = None, None\n\n    # Set model to evaluation mode\n    model.to(device)  # Move model to GPU\n    model.eval()\n\n    with torch.no_grad():\n        for _ in range(max_len):\n            output = model(input_tensor, target_seq)  # Forward pass\n            predicted_idx = output.argmax(dim=-1)[:, -1].item()  # Get the predicted word index\n            predicted_word = reverse_vocab_dict.get(predicted_idx, '<UNK>')\n\n            if predicted_word == '<end>':\n                break  # Stop when <end> token is generated\n            \n            generated_sequence.append(predicted_word)\n\n            # Update the target sequence with the predicted word\n            target_seq = torch.cat([target_seq, torch.tensor([[predicted_idx]], device=device)], dim=1)\n\n    return generated_sequence\n\n# Example usage of generating a sentence\n\ninput_seq = [\"ab\",\"jo\",\"palaT\",\"ke\",\"dekhiye\",\"thī\",\"kuchh\",\"muhāl\",\"bhī\"]\n  # Example input words\ngenerated_seq = generate_sequence(input_seq)\nprint(\"Generated Sequence:\", ' '.join(generated_seq))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:07:32.708859Z","iopub.execute_input":"2025-02-21T12:07:32.709161Z","iopub.status.idle":"2025-02-21T12:07:32.999220Z","shell.execute_reply.started":"2025-02-21T12:07:32.709140Z","shell.execute_reply":"2025-02-21T12:07:32.998520Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGenerated Sequence: ik aag sī vo bhī ab ke saath thī\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}